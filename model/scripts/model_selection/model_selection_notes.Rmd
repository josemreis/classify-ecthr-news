---
title: "ECtHR News Classifier"
subtitle: "model selection notes"
author: "J. M. Reis"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: no
    number_sections: yes
    theme: readable
    highlight: kate
    fig.height: 8
    fig.width: 10
    toc: yes
    toc_depth: 4
    css: "css_stylesheet.css"
link-citations: yes
---

```{r setup, echo=FALSE, cache=FALSE}
require(knitr)
## Global options
options(max.print="75")
opts_chunk$set(cache=TRUE,
               prompt=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```


# Intro

Training a classifier for identifying whether a (machine-translated) news articles is about a specific ecthr in our dataset, that is, for each ecthr ruling/news artice, identify whether the latter is about the former.

Load the packages.

```{r}
packs <- c("knitr", "rmdformats", "tidymodels", "textrecipes", "tidyverse", "themis", "ranger")
for (pack in packs){
  
  if (!pack %in% installed.packages()[,1]) {
    
    install.packages(pack)
    library(pack, character.only = TRUE)
    
  } else {
    
    library(pack, character.only = TRUE)
    
  }
  
}
```


# Relevant features

Load the model dataset...

```{r}
## load the latest model dataset
# keep features plus c
model_data <- read_csv("../../data/model_data.csv.gz") %>%
  mutate(model_id = paste(case_id, article_id, sep = "_"),
         ecthr_label = as_factor(ecthr_label)) %>%
  select(ecthr_label, text, article_nchar, date_distance, cosine_similarity_tf, jaccard_distance, lang_og = source_lang_alpha2, model_id)
glimpse(model_data)
```


## Ngrams represented as TF or TF-IDF vectors

These features will be generated below...

  1. We took each article and translated it to english using google translate [(unoffical) api](https://pypi.org/project/googletrans/).
  2. Next, we tokenize the text into ngrams from 1-4.
  3. As for pre-processing, will keep it simple. Only remove stop-words and punctuation. Practices such as stemming or lemmatization might be problematic in our context since, for example, verb conjugations are expected to be relatively relavant for identifying whether, say, the ruling mentioned is the actual one or parallel one close by.
    
## Ruling-article document similarity (original language or english translations)

The main idea is that the two documents, ruling and article, being close in their language should be indicative of whether the article covers it. String comparisons where done by comparing both as *document term matrices* where each is represented as a term-frequency normalized by document lenght (unique words) vector. 

TF-IDF seemed problematic in this instance given that the most relevant words were used as key-words, and thus would be probably excessively deflated. 

  1. We downloaded the rulings as well as ruling communication notices [e.g.](https://hudoc.echr.coe.int/fre?i=001-57619) in all language avaiable for each case.
  2. The choice of relevant document was done according to the following steps:
  
      2.1 If ruling is available in the original language $\to$ string comparison with article in original language; when missing.

      2.2. If ruling is available in english $\to$ string comparison with machine translated version of the article.
      
      2.3. If ruling is available in french (the other official language) machine translate it to english $\to$ string comparison with machine translated version of the article.
      
      2.4. If there is any ruling in any language available, take it and machine translate it $\to$ string comparison with machine translated version of the article.
      
      2.5. Repeat the steps above for communication notices. We found ruling docs for all cases in our dataset.
      
  3. For each dyad, we tokenized them using [a language specific tokenizers](https://stanfordnlp.github.io/stanza/) and extract morphological information for each word (*idem*). We first filtered the docs by pos-tags^[Namely, the following universal pos-tags were kept: "VERB", "ADJ", "ADP", "ADV", "DET", "AUX", "NOUN", "NUM", "PRON", "PROPN", "PART"; check https://universaldependencies.org/u/pos/]. Removed stop-words using a [language specific corpus](https://github.com/stopwords-iso/stopwords-iso). Tokenized them into 1-5 ngrams and computed their corpus length normalized term frequencies. 
  4. As for comparison, I first computed their [cosine similarity](https://www.sciencedirect.com/topics/computer-science/cosine-similarity) and then the [jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) between the tokens in each vector.

## Date distance

Took the difference in the number of days between the judgment date and the publication date for the article. Articles covering the ruling are expected to be more likely in the days leading to or following the ruling. Though on its own, its seems unlikely. Interacted, possibly.

```{r}
model_data %>%
  group_by(ecthr_label) %>%
  summarise(mean_dd = mean(date_distance)) %>%
  ungroup()
```

## Number of characters in article

Standard feature. Length of an article might indicate topic given current agenda cycle, though it may go in any direction.

## Source language

Language of the text before translation, ``source_lang_alpha2`` to capture language-related weaknesses in the machine translation and its consequences in the coding.

# Modeling

Split the data, 70% for training and the remaining for testing, stratified by article language. 

```{r}
set.seed(1234) 
the_split <- initial_split(model_data, strata = "lang_og", p = 0.70)
train_data <- training(the_split)
test_data <- testing(the_split)

train_data %>%
  group_by(ecthr_label) %>%
  summarise(n = n())

test_data %>%
  group_by(ecthr_label) %>%
  summarise(n = n())
```


### Class imbalance

Highly skewed classes tend to lead to models overperforming in predicting the majority class and underperforming in the minority class. Two ways around it:
  - Correction via oversampling of minority class, undersampling of the majority class, and via creating synthetic data for the minority class;
  - Cost-sensitive learning, i.e. assymetric weighting of miss-classification
The latter comes at the cost of loosing information from the training set, e.g. loosing words, thus comes at the cost of increasing the risk of our model overfitting the data^[[see](https://www.researchgate.net/publication/220705031_Cost-Sensitive_Learning_vs_Sampling_Which_is_Best_for_Handling_Unbalanced_Classes_with_Unequal_Error_Costs
)]. Will go with the second approach in the main workflow, though might also experiment with the former.


## Pre-processing

We start with the pre-processing. For this I will resort to ``recipes`` from tidymodels. This allows me to generate a pre-processing workflow which can be used in both training and testing datasets. For all the text-related pre-processing I will use ``textrecipes``, [details here](https://github.com/tidymodels/textrecipes).

Starting with the text. Will create three recipes. 

  1. tf_main:
  
  - tf vectors (1-4 ngrams)
  - no stopwords
  - tf > 20
  - main sampling
  - number of character natural logged
  - lang to dummies
  - random sample

```{r}
tf_main <- recipe(ecthr_label ~ ., data = train_data) %>%
  step_naomit(everything()) %>%
  step_filter(text != "" & !is.na(text)) %>%
  step_tokenize(text) %>%
  step_stopwords(text, keep = FALSE) %>%
  step_tokenfilter(text, min_times = 20) %>%
  step_untokenize(text) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 4, n_min = 1)) %>%
  step_tf(text) %>%
  step_log(article_nchar) %>%
  step_dummy(lang_og) %>%
  update_role(model_id, new_role = "id variable") %>%
  prep(training = train_data)

## the recipe
tf_main
str(juice(tf_main), list.len = 20)
```

    
  2. tfidf_main:
  
  - tfdf vectors (1-4 ngrams)
  - no stopwords
  - tf > 20
  - main sampling
  - number of character natural logged
  - random sample

```{r}
tfidf_main <- recipe(ecthr_label ~ ., data = train_data) %>%
  step_naomit(everything()) %>%
  step_filter(text != "" & !is.na(text)) %>%
  step_tokenize(text)  %>%
  step_stopwords(text, keep = FALSE) %>%
  step_tokenfilter(text, min_times = 20) %>%
  step_untokenize(text) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 4, n_min = 1)) %>%
  step_tfidf(text) %>%
  step_log(article_nchar) %>%
  step_dummy(lang_og) %>%
  update_role(model_id, new_role = "id variable") %>%
  prep(training = train_data)

## the recipe
tfidf_main
```

  
  3. tf_smote:
  
  - tfdf vectors (1-5 ngrams)
  - no stopwords
  - tf > 20
  - main sampling
  - number of character natural logged
  - "Synthetic Minority Over-sampling Technique" ([see](https://arxiv.org/abs/1106.1813))
  
```{r eval=FALSE, include=TRUE}
## BUG
tf_smote <- recipe(ecthr_label ~ ., data = train_data) %>%
  step_naomit(everything()) %>%
  step_filter(text != "" & !is.na(text)) %>%
  step_tokenize(text) %>%
  step_stopwords(text, keep = FALSE) %>%
  step_tokenfilter(text, min_times = 20) %>%
  step_untokenize(text) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 4, n_min = 1)) %>%
  step_tf(text) %>%
  step_log(article_nchar) %>%
  step_dummy(lang_og) %>%
  update_role(model_id, new_role = "id variable") %>%
  step_smote(ecthr_label, seed = 1234) %>%
  prep(training = train_data)

## the recipe
tf_smote
table(juice(tf_smote$ecthr_label))
```
    
  4. tf_main_stop:
  
  - tfdf vectors (1-5 ngrams)
  - keep stopwords
  - stemmed
  - main sampling
  - number of character natural logged
  - random sample

```{r}
tf_main_stop <- recipe(ecthr_label ~ ., data = train_data) %>%
  step_naomit(everything()) %>%
  step_filter(text != "" & !is.na(text)) %>%
  step_tokenize(text) %>%
  step_stopwords(text, keep = TRUE) %>%
  step_tokenfilter(text, min_times = 20) %>%
  step_untokenize(text) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 4, n_min = 1)) %>%
  step_tf(text) %>%
  step_log(article_nchar) %>%
  step_dummy(lang_og) %>%
  update_role(model_id, new_role = "id variable") %>%
  prep(training = train_data)

## the recipe
tf_main_stop
str(juice(tf_main_stop), list.len = 20)
```

  5. tf_main_langtokens:
  
  - tfdf vectors (1-5 ngrams)
  - no stopwords
  - language specific tokens, i.e., we concatenate the language of the original text to the text to capture translation idionsyncracies
  - main sampling
  - number of character natural logged
  - random sample


```{r eval=FALSE, include=TRUE}
## bug
tf_main_langtokens <- recipe(ecthr_label ~ ., data = train_data) %>%
  step_naomit(everything()) %>%
  step_filter(text != "" & !is.na(text)) %>%
  step_tokenize(text) %>%
  step_stopwords(text, keep = TRUE) %>%
  step_tokenfilter(text, min_times = 10) %>%
  step_untokenize(text) %>%
  step_mutate(text = paste(lang_og, text)) %>%
  step_rm(lang_og) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 4, n_min = 1)) %>%
  step_tf(text) %>%
  update_role(model_id, new_role = "id variable") %>%
  prep(training = train_data)

## the recipe
tf_main_langtokens
str(juice(tf_main_langtokens), list.len = 20)
```



