---
title: "ECtHR News Classifier"
subtitle: "model selection notes"
author: "J. M. Reis"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: no
    number_sections: yes
    theme: readable
    highlight: kate
    fig.height: 8
    fig.width: 10
    toc: yes
    toc_depth: 4
    css: "css_stylesheet.css"
link-citations: yes
---

```{r setup, echo=FALSE, cache=FALSE}
require(knitr)
## Global options
options(max.print="75")
opts_chunk$set(cache=TRUE,
               prompt=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```


# Intro

Training a classifier for identifying whether a (machine-translated) news articles is about a specific ecthr in our dataset, that is, for each ecthr ruling/news artice, identify whether the latter is about the former.

Load the packages.

```{r}
packs <- c("knitr", "rmdformats", "tidymodels", "textrecipes", "tidyverse", "themis", "ranger", "glmnet", "tidypredict")
for (pack in packs){
  
  if (!pack %in% installed.packages()[,1]) {
    
    install.packages(pack)
    library(pack, character.only = TRUE)
    
  } else {
    
    library(pack, character.only = TRUE)
    
  }
  
}
```


# Relevant features

Load the model dataset...

```{r}
## load the latest model dataset
# keep features plus c
model_data <- read_csv("../../data/model_data.csv.gz") %>%
  mutate(model_id = paste(case_id, article_id, sep = "_"),
         ecthr_label = as_factor(ecthr_label)) %>%
  select(ecthr_label, text, article_nchar, date_distance, cosine_similarity_tf, jaccard_distance, lang_og = source_lang_alpha2, contains("ner"), country_match_ratio, model_id) %>%
  drop_na()
glimpse(model_data)
```


## Ngrams represented as TF or TF-IDF vectors

These features will be generated below...

  1. We took each article and translated it to english using google translate [(unoffical) api](https://pypi.org/project/googletrans/).
  2. Next, we tokenize the text into ngrams from 1-4.
  3. As for pre-processing, will keep it simple. Only remove stop-words and punctuation. Practices such as stemming or lemmatization might be problematic in our context since, for example, verb conjugations are expected to be relatively relavant for identifying whether, say, the ruling mentioned is the actual one or parallel one close by.
    
## Ruling-article document similarity (original language or english translations)

The main idea is that the two documents, ruling and article, being close in their language should be indicative of whether the article covers it. String comparisons where done by comparing both as *document term matrices* where each is represented as a term-frequency normalized by document lenght (unique words) vector. 

TF-IDF seemed problematic in this instance given that the most relevant words were used as key-words, and thus would be probably excessively deflated. 

  1. We downloaded the rulings as well as ruling communication notices [e.g.](https://hudoc.echr.coe.int/fre?i=001-57619) in all language avaiable for each case.
  2. The choice of relevant document was done according to the following steps:
  
      2.1 If ruling is available in the original language $\to$ string comparison with article in original language; when missing.

      2.2. If ruling is available in english $\to$ string comparison with machine translated version of the article.
      
      2.3. If ruling is available in french (the other official language) machine translate it to english $\to$ string comparison with machine translated version of the article.
      
      2.4. If there is any ruling in any language available, take it and machine translate it $\to$ string comparison with machine translated version of the article.
      
      2.5. Repeat the steps above for communication notices. We found ruling docs for all cases in our dataset.
      
  3. For each dyad, we tokenized them using [a language specific tokenizers](https://stanfordnlp.github.io/stanza/) and extract morphological information for each word (*idem*). We first filtered the docs by pos-tags^[Namely, the following universal pos-tags were kept: "VERB", "ADJ", "ADP", "ADV", "DET", "AUX", "NOUN", "NUM", "PRON", "PROPN", "PART"; check https://universaldependencies.org/u/pos/]. Removed stop-words using a [language specific corpus](https://github.com/stopwords-iso/stopwords-iso). Tokenized them into 1-5 ngrams and computed their corpus length normalized term frequencies. 
  4. As for comparison, I first computed their [cosine similarity](https://www.sciencedirect.com/topics/computer-science/cosine-similarity) and then the [jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) between the tokens in each vector.

```{r}
model_data %>% group_by(ecthr_label) %>% summarise(mean_cos = IQR(cosine_similarity_tf), mean_jac = IQR(jaccard_distance))

model_data %>%
  ggplot(aes(x = ecthr_label, cosine_similarity_tf)) + 
  geom_boxplot() + 
  theme_minimal()


model_data %>%
  ggplot(aes(x = ecthr_label, jaccard_distance)) + 
  geom_boxplot() + 
  theme_minimal()
```

## NER ratios

  We further extracted recognized the named entities from the articles using [spacy](https://spacy.io/), counted their frequencies and normalized them by dividing the counts by the number of words in the text.

```{r}
model_data %>%
  select(contains("ner")) %>%
  glimpse()
```

## country match ratio

Similarly, we extracted all mentions of countries using the above-mentioned algorithm. Next, we counted the number of country matches with the country which was party with the case and normalized these counts by dividing it from all the country mentions.

```{r}
model_data %>%
  ggplot(aes(ecthr_label, country_match_ratio)) + 
  geom_boxplot() +
  theme_minimal()
```


## Date distance

Took the difference in the number of days between the judgment date and the publication date for the article. Articles covering the ruling are expected to be more likely in the days leading to or following the ruling. Though on its own, its seems unlikely.

```{r}
model_data %>%
  ggplot(aes(x = ecthr_label, date_distance)) + 
  geom_boxplot() + 
  theme_minimal()
```

  ## Number of characters in article

Standard feature. Length of an article might indicate topic given current agenda cycle, though it may go in any direction.

## Source language

Language of the text before translation, ``source_lang_alpha2`` to capture language-related weaknesses in the machine translation and its consequences in the coding.

# Modeling

## Sampling

Split the data, 70% for training and the remaining for testing, stratified by article language. 

```{r}
set.seed(1234) 
the_split <- initial_split(model_data, strata = "lang_og", p = 0.70)
train_data <- training(the_split)
test_data <- testing(the_split)

train_data %>%
  group_by(ecthr_label) %>%
  summarise(n = n())

test_data %>%
  group_by(ecthr_label) %>%
  summarise(n = n())
```

10-fold samples (stratified by the outcome variable) from the training data for cross-validation during tuning of the training models.

```{r}
# 10-fold cross-validation
set.seed(1234)
cv_samples <- vfold_cv(train_data, v = 10, repeats = 1, strata = "ecthr_label")
cv_samples
```


### Class imbalance


Highly skewed classes tend to lead to models overperforming in predicting the majority class and underperforming in the minority class. Two ways around it:
  - Correction via oversampling of minority class, undersampling of the majority class, and via creating synthetic data for the minority class;
  - Cost-sensitive learning, i.e. assymetric weighting of miss-classification
The latter comes at the cost of loosing information from the training set, e.g. loosing words, thus comes at the cost of increasing the risk of our model overfitting the data^[[see](https://www.researchgate.net/publication/220705031_Cost-Sensitive_Learning_vs_Sampling_Which_is_Best_for_Handling_Unbalanced_Classes_with_Unequal_Error_Costs
)]. Will go with the second approach in the main workflow, though might also experiment with the former.

## Pre-processing

We start with the pre-processing. For this I will resort to ``recipes`` from tidymodels. This allows me to generate a pre-processing workflow which can be used in both training and testing datasets. For all the text-related pre-processing I will use ``textrecipes``, [details here](https://github.com/tidymodels/textrecipes).

While stemming and stop-words removal may not be optimal, they seem to be the best alternative for reducing the dimensions of the text features while loosing the least information. Models without these steps constantly crashed. For the same reason, I restricted the ngrams to 1-3.

Starting with the text. Will create three recipes. 

  1. tf_main:
  
  - tf vectors (1-3 ngrams)
  - no stopwords
  - stemmed
  - tf > 2, max top words as tuneable
  - main sampling
  - number of character natural logged
  - lang to dummies
  - random sample


```{r}
tf_main <- recipe(ecthr_label ~ ., 
                  data = train_data) %>%
  update_role(model_id, new_role = "id variable") %>%
  step_dummy(lang_og) %>%
  step_log(article_nchar) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 3, n_min = 1, ngram_delim = "_")) %>%
  step_stem(text) %>%
  step_stopwords(text, keep = FALSE, stopword_source = "stopwords-iso") %>%
  step_tokenfilter(text, min_times = 2, max_tokens = 800)  %>%
  step_tf(text)
## the recipe
tf_main
```

    
  2. tfidf_main:
  
  - tf-idf vectors (1-3 ngrams)
  - no stopwords
  - stemmed
  - tf > 2, max top-words..
  - main sampling
  - number of character natural logged
  - lang to dummies
  - random sample


```{r}
tfidf_main <- recipe(ecthr_label ~ ., 
                  data = train_data) %>%
  update_role(model_id, new_role = "id variable") %>%
  step_dummy(lang_og) %>%
  step_log(article_nchar) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 3, n_min = 1, ngram_delim = "_")) %>%
  step_stem(text) %>%
  step_stopwords(text, keep = FALSE, stopword_source = "stopwords-iso") %>%
  step_tokenfilter(text, min_times = 2, max_tokens = 800)   %>%
  step_tfidf(text)
## the recipe
tfidf_main
```

  
  3. tfidf_upsample:
  
  - tf-idf vectors (1-3 ngrams)
  - no stopwords
  - stemmed
  - tf > 2, max top words as tuneable
  - main sampling
  - number of character natural logged
  - lang to dummies
  - For dealing with imbalance: upsample (https://link.springer.com/content/pdf/10.1007/s10618-012-0295-5.pdf)
  
  
```{r}
# tfidf_upsample <- recipe(ecthr_label ~ .,
#                    data = train_data) %>%
#   step_dummy(all_nominal(), -c(text, ecthr_label, model_id)) %>%
#   step_log(article_nchar) %>%
#   step_tokenize(text, token = "ngrams", options = list(n = 3, n_min = 1)) %>%
#   step_stem(text) %>%
#   step_stopwords(text, keep = FALSE, stopword_source = "stopwords-iso") %>%
#   step_tokenfilter(text, min_times = 2, max_tokens = 800)  %>%
#   step_tfidf(text) %>%
#   step_rm(model_id) %>%
#   step_upsample(ecthr_class, seed = 1234)
# 
# ## the recipe
# tfidf_upsample
```
    

## Bulding the models

The choice of models will be based on their (i) explainability (less black-box) and (ii) availability of implementation of cost-weights to compensate the class imbalance

### Penalized logistic regression (LASSO)

First, we fit a logistic regression penalized with a LASSO (least absolute shrinkage and selection operator)^[using ``glmnet``, [see](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)]] - adding a penalty term to the log likelihood function so as to shrink coefficients of the variables which contribute more to the error in te model to 0. Hence operating as feature selection. This should result in a more efficient model and less prone to overfitting. 


This is particularly useful for us since we will be using text-features too. The selected penalty term will be chosen based on its ability to minimize out of sample prediction errors, namely via grid search of a vector of several penalty terms (lamda) and using bootstrap cross-validation.


We will first, select the best model using the main recipe. We then select the best model from it, i.e. the best penalty term, and then we compare the performance of this model in the test-set accross different pre-processing recipes.

#### model

```{r}
### penalized logistic regression (lasso)
# penalty -> lambda; allowed to vary for hyperparameter tuning
# mixture = 1 for lasso regression
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```


#### Hyperparameter grid


```{r}
# tuning grid, range of 10 penalty values and max token values
param_grid <- grid_regular(penalty(),
                           levels = 20)

param_grid
```


#### Prepare the workflow

We will add ``step_normalize(all_predictors())``, i.e. normalize all predictors, as it is recommended


```{r}
## Prep the workflow: TF
lasso_wf_tf <- workflow() %>%
  add_recipe(
    tf_main %>% 
      step_normalize(all_predictors(), -article_nchar) # since it is logged
    ) %>%
  add_model(lasso_spec)

lasso_wf_tf

## Prep the workflow: TFIDF
lasso_wf_tfidf <- workflow() %>%
  add_recipe(
    tfidf_main %>% 
      step_normalize(all_predictors(), -article_nchar) # since it is logged
    ) %>%
  add_model(lasso_spec)

lasso_wf_tfidf

# ## Prep the workflow: upsample sampling
# lasso_wf_upsample <- workflow() %>%
#   add_recipe(
#     tfidf_upsample %>% 
#       step_normalize(all_predictors(), -article_nchar)
#     ) %>%
#   add_model(lasso_spec)
# 
# lasso_wf_upsample
```

#### Training and tunning the model

```{r}
### identify the best lambda via grid search
## TF
# parallelize
doParallel::registerDoParallel()
set.seed(1234)
lasso_grid_tf <- tune_grid(
  lasso_wf_tf,
  resamples = cv_samples,
  grid = param_grid,
  control = control_grid(verbose = FALSE, save_pred = TRUE),
  metrics = metric_set(roc_auc, bal_accuracy, f_meas, ppv, npv)
)

## plot the metrics
autoplot(lasso_grid_tf) +
theme_minimal() + 
ggtitle("tf model")

### Confusion matrices by roc-auc
lasso_grid_tf %>%
collect_predictions() %>%
filter(.config == select_best(lasso_grid_tf, metric = "roc_auc")$.config) %>%
conf_mat(ecthr_label, .pred_class)

### identify the best lambda via grid search
## TFIDF
# parallelize
doParallel::registerDoParallel()
set.seed(1234)
lasso_grid_tfidf <- tune_grid(
  lasso_wf_tfidf,
  resamples = cv_samples,
  grid = param_grid,
  control = control_grid(verbose = FALSE, save_pred = TRUE),
  metrics = metric_set(roc_auc, bal_accuracy, f_meas, ppv, npv)
)

## plot the metrics
autoplot(lasso_grid_tfidf) +
theme_minimal() + 
ggtitle("tfidf model")

### Confusion matrices by roc-auc
lasso_grid_tfidf %>%
collect_predictions() %>%
filter(.config == select_best(lasso_grid_tfidf, metric = "roc_auc")$.config) %>%
conf_mat(ecthr_label, .pred_class)

### identify the best lambda via grid search
## TFIDF upsample
# parallelize
# doParallel::registerDoParallel()
# set.seed(1234)
# lasso_grid_upsample <- tune_grid(
#   lasso_wf_upsample,
#   resamples = cv_samples,
#   grid = param_grid,
#   control = control_grid(verbose = FALSE, save_pred = TRUE),
#   metrics = metric_set(roc_auc, bal_accuracy, f_meas, ppv, npv)
# )
# 
# ## plot the metrics
# autoplot(lasso_grid_upsample) +
# theme_minimal() + 
# ggtitle("upsample model")
# 
# ### Confusion matrices by roc-auc
# lasso_grid_upsample %>%
# collect_predictions() %>%
# filter(.config == select_best(lasso_grid_upsample, metric = "roc_auc")$.config) %>%
# conf_mat(ecthr_label, .pred_class)

```



```{r}
# The positive predictive value (ppv()) is defined as the percent of predicted positives that are actually positive while the negative predictive value (npv()) is defined as the percent of negative positives that are actually negative.
# The positive predictive value (ppv()) is defined as the percent of predicted positives that are actually positive
show_best(lasso_grid_tfidf, "npv", n = 10)
#  the negative predictive value (npv()) is defined as the percent of negative positives that are actually negative
show_best(lasso_grid_tfidf, "ppv", n = 10)
# average of the precision and recall
show_best(lasso_grid_tfidf, "bal_accuracy", n = 10)
show_best(lasso_grid_tfidf, "roc_auc", n = 10)
show_best(lasso_grid_tfidf, "f_meas", n = 10)
```


#### Select best model and finalize the workflow


```{r}
## pull the best model specification using roc_auc metric
models <- list("tf" = lasso_grid_tf, 
                           "tfidf" = lasso_grid_tfidf)

best_params <- map2_df(models, names(models),
                      ~ show_best(.x, metric = "roc_auc") %>%
                        mutate(model_type = .y))

## slice the best parameters
(final_model <- best_params %>%
  arrange(desc(mean)) %>%
  slice(1))

## select the best workflow
if (final_model$model_type[1] == "tf") {
  
  final_lasso_wf <- finalize_workflow(
  lasso_wf_tf,
  final_model %>% select(penalty, .config)
  )
  
} else {
  
  final_lasso_wf <- finalize_workflow(
  lasso_wf_tfidf,
  final_model %>% select(penalty, .config)
  )
  
} 

```


#### Evaluate on the test-set

```{r}
lasso_fit <- final_lasso_wf %>%
  last_fit(the_split)
```


```{r}
### evaluate with metrics
test_performance <- lasso_fit %>% collect_metrics()
test_performance
```

```{r}
# generate predictions from the test set
test_predictions <- lasso_fit %>% collect_predictions()

## confusion matrix
test_predictions %>%
  conf_mat(ecthr_label, .pred_class)

```


#### Fitting the final model

Fit the final model on the entire trained dataset.

```{r}
final_model <- fit(final_lasso_wf, model_data)

final_model
```


#### Variable importance

```{r}
lasso_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 40) + 
  theme_minimal()
```

#### Export the final model

```{r eval=FALSE, include=TRUE}

write_rds(final_model,
          path =  "models/lasso_model.Rds",
          compress = "gz")

write_rds(lasso_fit,
          path =  "models/lasso_workflow.Rds",
          compress = "gz")
```


### Random forests

Next, a random forests models following ``ranger``'s implementation. First we will implement one attributing the same costs for all miss-classifications and later we will implement a cost sensitive one.

#### Models

```{r}
### Baseline
rf_base <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 1000, # number of randomly samped predictor in each split when creating the trees
) %>%
  set_mode("classification") %>%
  set_engine(
    engine = "ranger",
    seed = 1234,
    importance = "impurity",
    splitrule = "gini",
    num.threads = 4
  )

### Cost-sensitive
rf_cs <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 1000, # number of randomly samped predictor in each split when creating the trees
) %>%
  set_mode("classification") %>%
  set_engine(
    engine = "ranger",
    seed = 1234,
    num.threads = 4,
    importance = "impurity",
    class.weights = c(0.01, 1) # cost sensitive learning, higher penalty for minority class)
  )
```


#### Hyperparameter grid and bootstrap samples

Tunable hyperparameters^[[see](https://arxiv.org/pdf/1804.03515.pdf)]:
  - The number of randomly drawn candidate variables out of which each split is selected when growing a tree (mtry). Lower values of mtry lead to less correlated trees which is good, however this might lead to decreases in average accuracy as the probability of not including a relevant variable increases. Traditionally, the default value proposed for balancing these concerns is $mtry=\sqrt{\#features}$.
  - Node size (min_n). The node size parameter specifies the minimum number of observations in a terminal node. Low values lead to trees with a larger depth and so more splits are performed until the terminal nodes.
  

```{r}
# tuning grid
param_grid <- grid_regular(mtry(range = c(50, 100)),
                           min_n(range(2, 60)),
                           levels = 20)

param_grid

```


#### Prepare the workflows

```{r}

if (exists("tfidf_main")) {
  
  final_workflow <- tfidf_main
  
} else {
  
  final_workflow <- recipe(ecthr_label ~ ., 
                  data = train_data) %>%
  update_role(model_id, new_role = "id variable") %>%
  step_dummy(lang_og) %>%
  step_log(article_nchar) %>%
  step_tokenize(text, token = "ngrams", options = list(n = 3, n_min = 1, ngram_delim = "_")) %>%
  step_stem(text) %>%
  step_stopwords(text, keep = FALSE, stopword_source = "stopwords-iso") %>%
  step_tokenfilter(text, min_times = 2, max_tokens = 800)   %>%
  step_tfidf(text)
  
}

## Prep the workflow
rf_base_wf <- workflow() %>%
  add_recipe(
    final_workflow %>% 
      step_normalize(all_predictors(), -article_nchar) # since it is logged
    ) %>%
  add_model(rf_base)

rf_base_wf

rf_cs_wf <- workflow() %>%
  add_recipe(
    final_workflow %>% 
      step_normalize(all_predictors(), -article_nchar) # since it is logged
    ) %>%
  add_model(rf_cs)

rf_cs_wf
```


#### Training and tunning the model

```{r}
### train
## baseline
# parallelize
doParallel::registerDoParallel()
set.seed(1234)
rf_grid_base <- tune_grid(
  rf_base_wf,
  resamples = cv_samples,
  grid = param_grid,
  control = control_grid(verbose = FALSE, save_pred = TRUE),
  metrics = metric_set(roc_auc, bal_accuracy, f_meas, ppv, npv)
)

## plot the metrics
autoplot(rf_grid_base) +
theme_minimal()

### Confusion matrices by roc-auc
rf_grid_base %>%
collect_predictions() %>%
filter(.config == select_best(rf_grid_base, metric = "roc_auc")$.config) %>%
conf_mat(ecthr_label, .pred_class)

### train
## cost-sensitive
# parallelize
doParallel::registerDoParallel()
set.seed(1234)
rf_grid_cs <- tune_grid(
  rf_cs_wf,
  resamples = cv_samples,
  grid = param_grid,
  control = control_grid(verbose = FALSE, save_pred = TRUE),
  metrics = metric_set(roc_auc, bal_accuracy, f_meas, ppv, npv)
)

## plot the metrics
autoplot(rf_grid_base) +
theme_minimal()

### Confusion matrices by roc-auc
rf_grid_cs %>%
collect_predictions() %>%
filter(.config == select_best(rf_grid_cs, metric = "roc_auc")$.config) %>%
conf_mat(ecthr_label, .pred_class)
```


#### Select best model and finalize the workflow


```{r eval=FALSE, include=FALSE}
## pull the best model specification using cohen's kappa as the relevant metric
models <- list("tf" = lasso_grid_tf, 
                           "tfidf" = lasso_grid_tfidf, 
                           "upsample" = lasso_grid_upsample)

best_params <- map2_df(models, names(models),
                      ~ show_best(.x, metric = "roc_auc") %>%
                        mutate(model_type = .y))

## slice the best parameters
(final_model <- best_params %>%
  arrange(desc(mean)) %>%
  slice(1))

## select the best workflow
if (final_model$model_type[1] == "tf") {
  
  final_lasso_wf <- finalize_workflow(
  lasso_wf_tf,
  final_model %>% select(penalty, .config)
  )
  
} else if (final_model$model_type[1] == "tfidf") {
  
  final_lasso_wf <- finalize_workflow(
  lasso_wf_tfidf,
  final_model %>% select(penalty, .config)
  )
  
} else {
  
  final_lasso_wf <- finalize_workflow(
  lasso_wf_tf,
  final_model %>% select(penalty, .config)
  )
  
}

```


  