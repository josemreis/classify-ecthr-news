prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 80, 20),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -4, 2),
sigma = runif(n, 5, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 80, 20),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -4, 2),
sigma = runif(n, 10, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 80, 20),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -4, 4),
sigma = runif(n, 10, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 80, 20),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -4, 4),
sigma = runif(n, 10, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 80, 20),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = runif(n, 10, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 60, 10),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = runif(n, 10, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 60, 10),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = runif(n, 0, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 60, 10),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = rnorm(n, 40, 10),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 60, 10),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = rnorm(n, 10, 5),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 60, 10),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = runif(n, 0, 10),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 60, 10),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = runif(n, 10, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
set.seed(1234)
n <- 1e3
## generate the prior predictive distribution with the baseline priors
prior_pred <- tibble(alpha = rnorm(n, 80, 20),
beta_1 = rlnorm(n, 3, 0.4),
beta_2 = rnorm(n, -5, 5),
sigma = runif(n, 10, 20),
weight = seq(4, 80, length.out = n)) %>%
mutate(weight_s = scale(weight, center = TRUE, scale = TRUE),
weight_s2 = scale(weight^2, center = TRUE, scale = TRUE),
expected_mu = alpha + beta_1 * weight_s + beta_2 * weight_s2,
predicted_heights = rnorm(
n = n,
mean = expected_mu,
sd = sigma
))
prior_pred %>%
ggplot(aes(x = weight, y = predicted_heights)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm")
dirname(parent.frame(2)$ofile)
parent.frame(2)$ofile
parent.frame
parent.frame(1)
dirname(rstudioapi::getActiveDocumentContext()$path)
### wd at file location
parent_dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(parent_dir)
### Move deccano api helpers script to helpers folder
file.copy(
from = '/home/jmr/Dropbox/python_default/doccano_api_helpers/scripts/doccano_helpers.py',
to = 'scripts/helpers/doccano_helpers.py'
)
### dirs
if (!dir.exists("scripts")) {
dir.create("scripts")
if (!dir.exists("scripts/helpers")) {
dir.create("scripts/helpers")
}
}
if (!dir.exists("data")) {
dir.create("data")
}
### Move deccano api helpers script to helpers folder
file.copy(
from = '/home/jmr/Dropbox/python_default/doccano_api_helpers/scripts/doccano_helpers.py',
to = 'scripts/helpers/doccano_helpers.py'
)
### Packs
require(tidyverse)
#### Setting things up
#### ---------------------------------------------------------------------------
### packages
require(tidyverse)
require(lubridate)
require(data.table)
require(furrr)
#### Add some missing metadata------------------------------------------------------------------------------
### multi core
plan(multiprocess)
## data collection dir
parent_dir <-  '/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/3_classify_ecthr_news'
data_dir <- '/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/wrangle'
setwd(data_dir)
#### Load the datasets
#### --------------------------------------------------------------------------------------
### ** prep factiva **
### path for the corpus factiva dataset
factiva_path <- "/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/factiva/scraper/data/1_factiva_ecthr_mediacoverage.csv.gz"
## load and standardize variable names. For corpus data, keep only text related vars pus case_id and article_id as keys
factiva_corpus <- read_csv(factiva_path) %>%
select(case_id,
article_id = ecthr_article_id,
article_title = ecthr_article_title,
article_leading_paragraph = ecthr_article_leadingParagraph,
article_maintext = ecthr_article_remainingContent,
source_language = ecthr_article_language) %>%
mutate(article_maintext = ifelse(is.na(article_maintext),
article_leading_paragraph,
article_maintext),
ecthr_match = "1") %>% # remove duplicates
distinct(article_id, article_leading_paragraph, .keep_all = TRUE)
### ** prep alternative sources **
## path for the corpus alternative sources datasets
alt_corpus_paths <- Sys.glob("/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/alternative_sources/scrape/country-level/*/data/2_*.csv.gz")
### load all datasets into a list and then into a dataframe.
alt_sources_corpus <- furrr::future_map_dfr(alt_corpus_paths, function(path) {
## load it and filter
loaded <- try(read_csv(path) %>%
select(ecthr_match,
case_id,
article_id = contains("article_id"),
article_title = contains("article_title"),
article_leading_paragraph = one_of("article_leading", "article_description"),
article_maintext = one_of("article_remaining", "article_maintext"),
source_language = contains("language")) %>% # remove duplicates
distinct(article_id, article_leading_paragraph, .keep_all = TRUE), silent = TRUE)
if (class(loaded) == "try-error") {
print(loaded)
} else {
return(loaded)
}
})
## stop parallel clusters
future:::ClusterRegistry("stop")
#### Combine --------------------------------------------------------------------------------------------------
## rbind them
corpus_all <- rbind(factiva_corpus, alt_sources_corpus)
# rm(list=ls())
#### Select the relevant data for handcoding --------------------------------------------------------------------------
### Get all matches, keep only leading paragraphand article_id and main text
all_matches <- corpus_all %>%
filter(ecthr_match == "1") %>%
distinct(article_id, .keep_all = TRUE)
#### Setting things up
#### ---------------------------------------------------------------------------
### packages
require(tidyverse)
require(lubridate)
require(data.table)
require(furrr)
#### Add some missing metadata------------------------------------------------------------------------------
### multi core
plan(multiprocess)
## data collection dir
parent_dir <-  '/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/3_classify_ecthr_news'
data_dir <- '/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/2_wrangle'
setwd(data_dir)
#### Load the datasets
#### --------------------------------------------------------------------------------------
### ** prep factiva **
### path for the corpus factiva dataset
factiva_path <- "/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/1_data_collection/factiva/scraper/data/1_factiva_ecthr_mediacoverage.csv.gz"
## load and standardize variable names. For corpus data, keep only text related vars pus case_id and article_id as keys
factiva_corpus <- read_csv(factiva_path) %>%
select(case_id,
article_id = ecthr_article_id,
article_title = ecthr_article_title,
article_leading_paragraph = ecthr_article_leadingParagraph,
article_maintext = ecthr_article_remainingContent,
source_language = ecthr_article_language) %>%
mutate(article_maintext = ifelse(is.na(article_maintext),
article_leading_paragraph,
article_maintext),
ecthr_match = "1") %>% # remove duplicates
distinct(article_id, article_leading_paragraph, .keep_all = TRUE)
### ** prep alternative sources **
## path for the corpus alternative sources datasets
alt_corpus_paths <- Sys.glob("/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/alternative_sources/scrape/country-level/*/data/2_*.csv.gz")
### load all datasets into a list and then into a dataframe.
alt_sources_corpus <- furrr::future_map_dfr(alt_corpus_paths, function(path) {
## load it and filter
loaded <- try(read_csv(path) %>%
select(ecthr_match,
case_id,
article_id = contains("article_id"),
article_title = contains("article_title"),
article_leading_paragraph = one_of("article_leading", "article_description"),
article_maintext = one_of("article_remaining", "article_maintext"),
source_language = contains("language")) %>% # remove duplicates
distinct(article_id, article_leading_paragraph, .keep_all = TRUE), silent = TRUE)
if (class(loaded) == "try-error") {
print(loaded)
} else {
return(loaded)
}
})
## stop parallel clusters
future:::ClusterRegistry("stop")
## rbind them
corpus_all <- rbind(factiva_corpus, alt_sources_corpus)
# rm(list=ls())
#### Select the relevant data for handcoding --------------------------------------------------------------------------
### Get all matches, keep only leading paragraphand article_id and main text
all_matches <- corpus_all %>%
filter(ecthr_match == "1") %>%
distinct(article_id, .keep_all = TRUE)
View(all_matches)
#### Setting things up
#### ---------------------------------------------------------------------------
### packages
require(tidyverse)
require(lubridate)
require(data.table)
require(furrr)
#### Add some missing metadata------------------------------------------------------------------------------
### multi core
plan(multiprocess)
## data collection dir
parent_dir <-  '/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/3_classify_ecthr_news'
data_dir <- '/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/2_wrangle'
setwd(data_dir)
#### Load the datasets
#### --------------------------------------------------------------------------------------
### ** prep factiva **
### path for the corpus factiva dataset
factiva_path <- "/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/1_data_collection/factiva/scraper/data/1_factiva_ecthr_mediacoverage.csv.gz"
## load and standardize variable names. For corpus data, keep only text related vars pus case_id and article_id as keys
factiva_corpus <- read_csv(factiva_path) %>%
select(case_id,
article_id = ecthr_article_id,
article_title = ecthr_article_title,
article_leading_paragraph = ecthr_article_leadingParagraph,
article_maintext = ecthr_article_remainingContent,
source_language = ecthr_article_language) %>%
mutate(article_maintext = ifelse(is.na(article_maintext),
article_leading_paragraph,
article_maintext),
ecthr_match = "1") %>% # remove duplicates
distinct(article_id, article_leading_paragraph, .keep_all = TRUE)
### ** prep alternative sources **
## path for the corpus alternative sources datasets
alt_corpus_paths <- Sys.glob("/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/1_data_collection/alternative_sources/scrape/country-level/*/data/2_*.csv.gz")
### load all datasets into a list and then into a dataframe.
alt_sources_corpus <- furrr::future_map_dfr(alt_corpus_paths, function(path) {
## load it and filter
loaded <- try(read_csv(path) %>%
select(ecthr_match,
case_id,
article_id = contains("article_id"),
article_title = contains("article_title"),
article_leading_paragraph = one_of("article_leading", "article_description"),
article_maintext = one_of("article_remaining", "article_maintext"),
source_language = contains("language")) %>% # remove duplicates
distinct(article_id, article_leading_paragraph, .keep_all = TRUE), silent = TRUE)
if (class(loaded) == "try-error") {
print(loaded)
} else {
return(loaded)
}
})
## stop parallel clusters
future:::ClusterRegistry("stop")
## rbind them
corpus_all <- rbind(factiva_corpus, alt_sources_corpus)
# rm(list=ls())
#### Select the relevant data for handcoding --------------------------------------------------------------------------
### Get all matches, keep only leading paragraphand article_id and main text
all_matches <- corpus_all %>%
filter(ecthr_match == "1") %>%
distinct(article_id, .keep_all = TRUE)
### load the translated docs and put them together
listed_files <- list.files("data/interm_data/translated", full.names = TRUE)
### multi core
plan(multiprocess)
### Combine
ecthr_trans <- future_map(listed_files, ~read_csv(.x)) %>%
data.table::rbindlist(.) %>%
as_tibble()
## stop parallel clusters
future:::ClusterRegistry("stop")
future_map(listed_files, ~read_csv(.x)) %>%
data.table::rbindlist(.)
plan(multiprocess)
ecthr_trans <- future_map(listed_files, ~read_csv(.x))
listed_files
listed_files
listed_files
getwd()
data_dir <- '/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/2_wrangle'
setwd(data_dir)
listed_files <- list.files("data/interm_data/translated", full.names = TRUE)
getwd()
data_dir
listed_files <- list.files("/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/2_wrangle/data/interm_data/translated", full.names = TRUE)
listed_files <- list.files('/home/jmr/Dropbox/Current projects/thesis_papers/transparency, media, and compliance with HR Rulings/ecthr_media&compliance/data/media_data/2_wrangle_corpus/data/interm_data/translated', full.names = TRUE)
### multi core
plan(multiprocess)
### Combine
ecthr_trans <- future_map(listed_files, ~read_csv(.x)) %>%
data.table::rbindlist(.) %>%
as_tibble()
## stop parallel clusters
future:::ClusterRegistry("stop")
glimpse(corpus_all)
